{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dba03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(texts):\n",
    "    cleaned_texts = []\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for text in texts:\n",
    "        doc = nlp(text.lower())\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article!\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and w.text != 'I':\n",
    "                # we add the lematized version of the word\n",
    "                word = w.lemma_.strip()\n",
    "                # do we want = > <\n",
    "                if word != '' and word != '=' and word != \">\" and word != \"<\" and word not in stop_words:\n",
    "                    cleaned_text.append(word)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "        cleaned_text = []\n",
    "        \n",
    "    bigram = gensim.models.Phrases(cleaned_texts)\n",
    "    cleaned_texts = [bigram[line] for line in cleaned_texts]\n",
    "    cleaned_words = []\n",
    "    for text in cleaned_texts:\n",
    "        for word in text:\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summaries(texts):\n",
    "    total_docs = len(texts)\n",
    "    avg_len = 0\n",
    "    for text in texts:\n",
    "        avg_len += len(text)\n",
    "    avg_len /= total_docs\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_texts.append(clean_texts(text))\n",
    "    \n",
    "    word_use = {}\n",
    "    for text in cleaned_texts:\n",
    "        for word in text:\n",
    "            if word not in word_use:\n",
    "                word_use[word] = 0\n",
    "            if word in word_use:\n",
    "                word_use[word] += 1\n",
    "    \n",
    "    sorted_words = sorted(word_use.items(), key=operator.itemgetter(1))\n",
    "    sorted_words.reverse()\n",
    "\n",
    "    dictionary = Dictionary(cleaned_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in cleaned_texts]\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10, iterations=500)\n",
    "    \n",
    "    tf_idf_model = TfidfModel(corpus)\n",
    "    tf_idf_texts = tf_idf_model[corpus]\n",
    "    \n",
    "    d = {dictionary.get(id): value for doc in tf_idf_texts for id, value in doc}\n",
    "    sorted_d = sorted(d.items(), key=operator.itemgetter(1))\n",
    "    sorted_d.reverse()\n",
    "    \n",
    "    print(\"Total number of documents: \" + str(total_docs))\n",
    "    print(\"Average length of text: \" + str(avg_len))\n",
    "    print(\" \")\n",
    "    print(\"The top 5 tf-idf scores:\")\n",
    "    print(sorted_d[0:5])\n",
    "    print(\" \")\n",
    "    print(\"The top 20 most used words:\")\n",
    "    print(sorted_words[0:20])\n",
    "    print(\" \")\n",
    "    print(\"Topic Model with 2 topics\")\n",
    "    print(ldamodel.print_topics())\n",
    "    return cleaned_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joined_texts(texts):\n",
    "    joined_texts = []\n",
    "    for text in texts:\n",
    "        s = ' '\n",
    "        joined_text = s.join(text)\n",
    "        joined_texts.append(joined_text)\n",
    "    return joined_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use with embedding model\n",
    "def sentence_entropy(sentence, model):\n",
    "    sentence_word_vectors = []\n",
    "    if len(sentence) >= 2:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                sentence_word_vectors.append(model[word])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        distances = []\n",
    "        for i in range(0, len(sentence_word_vectors) - 1):\n",
    "            v = sentence_word_vectors[i+1]\n",
    "            v_ = sentence_word_vectors[i]\n",
    "            dist = np.dot(v, v_)/(np.linalg.norm(v)* np.linalg.norm(v_))\n",
    "            distances.append(dist)\n",
    "        mean_distance = np.mean(distances, axis=0)\n",
    "        semantic_variability = 0\n",
    "        for dist in distances:\n",
    "            semantic_variability += np.square(dist - mean_distance)\n",
    "        semantic_variability /= len(distances)\n",
    "        return semantic_variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3328c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use with BERT like language model\n",
    "def get_sentence_perplexity(sentence, model, tokenizer):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions = model(tensor_input)\n",
    "    # print(predictions[0])\n",
    "    # sentence_embedding, word_embedding = predictions[0], predictions[1]\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(predictions[0].squeeze(),tensor_input.squeeze()).data \n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8508b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use with GPT like model\n",
    "def gpt_ppl_score(sentence, model, tokenizer, stride=512):\n",
    "    encodings = tokenizer(sentence, return_tensors='pt')\n",
    "#     encodings = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    max_length = model.config.n_positions\n",
    "    lls = []\n",
    "    for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i    # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:,begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:,:-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        lls.append(log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    return ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
